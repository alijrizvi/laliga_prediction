{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Bar Graph Visualizing Each Team's Chances of Winning the League in 2024-2025:\n",
        "\n",
        "After the Random Forests Classifier was Implemented."
      ],
      "metadata": {
        "id": "2MrkJWlxxGWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Sort by \"projected_pts\" (Descending)\n",
        "df_test_sorted = df_test.sort_values('projected_pts', ascending = False)\n",
        "\n",
        "# Plotting the Bar Chart\n",
        "fig = px.bar(\n",
        "    df_test_sorted,\n",
        "    x = 'squad',\n",
        "    y = 'win_probability (%)',\n",
        "    color_discrete_sequence = [\"#1f77b4\"],\n",
        "    title = \"Championship Win Probabilities (2024-2025 Season)\",\n",
        "    labels = {'squad': 'Team', \"win_probability (%)\": \"Win Probability (%)\"},\n",
        "    template = \"plotly_white\"\n",
        ")\n",
        "\n",
        "# Improving the Layout\n",
        "fig.update_layout(\n",
        "    xaxis_title = 'Team',\n",
        "    yaxis_title = 'Win Probability (%)',\n",
        "    xaxis_tickangle = -60,\n",
        "    bargap = 0.2,\n",
        "    title_x = 0.5\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QHNXo4oExF_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Technique #1: L1 (Lasso) Regularization for Logistic Regression"
      ],
      "metadata": {
        "id": "0_wqd8wk-iPT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XctFbbx-Z_R"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Getting the Best 'C' - Inversely Related to Regularization Strength\n",
        "params_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "grid = GridSearchCV(LogisticRegression(penalty = 'l1', solver = 'liblinear', max_iter = 1000),\n",
        "                    params_grid, cv = 5) # Ridge: penalty = 'l2', solver = 'lbfgs'\n",
        "grid.fit(X_train_sc, y_train)\n",
        "# print(\"Best C:\", grid.best_params_['C'])\n",
        "\n",
        "\n",
        "# L1 Regularization (Lasso)\n",
        "\n",
        "logreg_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear', C = grid.best_params_['C'], max_iter = 1000)\n",
        "logreg_l1.fit(X_train_sc, y_train)\n",
        "print(\"L1 (Lasso) Regularization Results:\", classification_report(y_test, logreg_l1.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Technique #2: L2 (Ridge) Regularization for Logistic Regression"
      ],
      "metadata": {
        "id": "9wVeQICTxcK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 would be the same^ exact; just with l2 in place and solver = 'lbfgs'\n",
        "\n",
        "logreg_l2 = LogisticRegression(penalty = 'l2', solver = 'lbfgs', C = grid.best_params_['C'], max_iter = 1000)\n",
        "logreg_l2.fit(X_train_sc, y_train)\n",
        "print(\"L2 (Ridge) Regularization Results:\", classification_report(y_test, logreg_l2.predict(X_test)))"
      ],
      "metadata": {
        "id": "x3UoCaKBxc0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional Technique #3: Elastic-Net Regularization for Logistic Regression"
      ],
      "metadata": {
        "id": "BoXvDkO4xklU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elastic-Net Regression\n",
        "\n",
        "logreg_en = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5, C = grid.best_params_['C'], max_iter = 1000)\n",
        "logreg_en.fit(X_train_sc, y_train)\n",
        "\n",
        "# Can tweak \"l1_ratio\" - the closer to 0; more of Ridge (L2), the closer to 1; more of Lasso (L1)"
      ],
      "metadata": {
        "id": "UNhpdyU0xk36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional Technique #2: Visualizing the Residuals of the Lasso Regression Model:"
      ],
      "metadata": {
        "id": "-iSJwY9M-4Nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "res = y_test_linreg - y_pred_lassocv\n",
        "\n",
        "df_resids = pd.DataFrame({'Actual': y_test_linreg, 'Predicted': y_pred_lassocv, 'Residuals': res})\n",
        "\n",
        "plt.figure(figsize = (10, 7))\n",
        "sns.regplot(data = df_resids, x = 'Predicted', y = 'Residuals', line_kws = {'color': 'blue', 'linestyle': '-'})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "66IP4_Z7-7l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Can check for Multicollinearity between Features by:\n",
        "*from statsmodels.stats.outliers_influence import variance_inflation_factor*..."
      ],
      "metadata": {
        "id": "5tam72Dh-_Ab"
      }
    }
  ]
}